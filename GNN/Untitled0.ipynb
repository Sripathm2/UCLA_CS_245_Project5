{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPqvENG2oj8b7OLQ9OZc790",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sripathm2/UCLA_CS_245_Project5/blob/GNN/GNN/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oTGuos50JVi"
      },
      "source": [
        "%pip install spektral"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R7aeyLp0Q0b",
        "outputId": "7f40c19d-765f-4fc4-ab58-144beb88fb60"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "from spektral.models import GeneralGNN\r\n",
        "\r\n",
        "from spektral.data import DisjointLoader\r\n",
        "\r\n",
        "from spektral.datasets import TUDataset\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "\r\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\r\n",
        "if len(physical_devices) > 0:\r\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n",
        "\r\n",
        "# Best config\r\n",
        "batch_size = 32\r\n",
        "learning_rate = 0.01\r\n",
        "epochs = 1\r\n",
        "\r\n",
        "# Read data\r\n",
        "data = TUDataset('PROTEINS')\r\n",
        "\r\n",
        "# Train/test split\r\n",
        "np.random.shuffle(data)\r\n",
        "split = int(0.8 * len(data))\r\n",
        "data_tr, data_te = data[:split], data[split:]\r\n",
        "\r\n",
        "# Data loader\r\n",
        "loader_tr = DisjointLoader(data_tr, batch_size=batch_size, epochs=epochs)\r\n",
        "loader_te = DisjointLoader(data_te, batch_size=batch_size)\r\n",
        "\r\n",
        "# Create model\r\n",
        "model = GeneralGNN(data.n_labels, activation='softmax')\r\n",
        "optimizer = Adam(learning_rate)\r\n",
        "model.compile('adam', 'categorical_crossentropy', metrics=['categorical_accuracy'])\r\n",
        "\r\n",
        "\r\n",
        "# Evaluation function\r\n",
        "def evaluate(loader):\r\n",
        "    step = 0\r\n",
        "    results = []\r\n",
        "    for batch in loader:\r\n",
        "        step += 1\r\n",
        "        loss, acc = model.test_on_batch(*batch)\r\n",
        "        results.append((loss, acc))\r\n",
        "        if step == loader.steps_per_epoch:\r\n",
        "            return np.mean(results, 0)\r\n",
        "\r\n",
        "\r\n",
        "# Training loop\r\n",
        "epoch = step = 0\r\n",
        "results = []\r\n",
        "for batch in loader_tr:\r\n",
        "    step += 1\r\n",
        "    loss, acc = model.train_on_batch(*batch)\r\n",
        "    results.append((loss, acc))\r\n",
        "    if step == loader_tr.steps_per_epoch:\r\n",
        "        step = 0\r\n",
        "        epoch += 1\r\n",
        "        results_te = evaluate(loader_te)\r\n",
        "        print('Epoch {} - Train loss: {:.3f} - Train acc: {:.3f} - '\r\n",
        "              'Test loss: {:.3f} - Test acc: {:.3f}'\r\n",
        "              .format(epoch, *np.mean(results, 0), *results_te))\r\n",
        "\r\n",
        "results_te = evaluate(loader_te)\r\n",
        "print('Final results - Loss: {:.3f} - Acc: {:.3f}'.format(*results_te))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading PROTEINS dataset.\n",
            "Successfully loaded PROTEINS.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 - Train loss: 0.649 - Train acc: 0.636 - Test loss: 0.608 - Test acc: 0.695\n",
            "Final results - Loss: 0.606 - Acc: 0.695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB-VnlUzPc3v"
      },
      "source": [
        "from tensorflow.keras import layers\r\n",
        "from tensorflow import keras\r\n",
        "\r\n",
        "class Sampling(layers.Layer):\r\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        z_mean, z_log_var = inputs\r\n",
        "        batch = tf.shape(z_mean)[0]\r\n",
        "        dim = tf.shape(z_mean)[1]\r\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\r\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\r\n",
        "\r\n",
        "\r\n",
        "class Encoder(layers.Layer):\r\n",
        "    \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, latent_dim=32, intermediate_dim=64, name=\"encoder\", **kwargs):\r\n",
        "        super(Encoder, self).__init__(name=name, **kwargs)\r\n",
        "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\r\n",
        "        self.dense_mean = layers.Dense(latent_dim)\r\n",
        "        self.dense_log_var = layers.Dense(latent_dim)\r\n",
        "        self.sampling = Sampling()\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        x = self.dense_proj(inputs)\r\n",
        "        z_mean = self.dense_mean(x)\r\n",
        "        z_log_var = self.dense_log_var(x)\r\n",
        "        z = self.sampling((z_mean, z_log_var))\r\n",
        "        return z_mean, z_log_var, z\r\n",
        "\r\n",
        "\r\n",
        "class Decoder(layers.Layer):\r\n",
        "    \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, original_dim, intermediate_dim=64, name=\"decoder\", **kwargs):\r\n",
        "        super(Decoder, self).__init__(name=name, **kwargs)\r\n",
        "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\r\n",
        "        self.dense_output = layers.Dense(original_dim, activation=\"sigmoid\")\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        print(type(self.dense_proj))\r\n",
        "        print(inputs)\r\n",
        "        x = self.dense_proj(inputs)\r\n",
        "        return self.dense_output(x)\r\n",
        "\r\n",
        "\r\n",
        "class VariationalAutoEncoder(keras.Model):\r\n",
        "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        original_dim,\r\n",
        "        intermediate_dim=64,\r\n",
        "        latent_dim=32,\r\n",
        "        name=\"autoencoder\",\r\n",
        "        **kwargs\r\n",
        "    ):\r\n",
        "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\r\n",
        "        self.original_dim = original_dim\r\n",
        "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\r\n",
        "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\r\n",
        "        reconstructed = self.decoder(z)\r\n",
        "        # Add KL divergence regularization loss.\r\n",
        "        kl_loss = -0.5 * tf.reduce_mean(\r\n",
        "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\r\n",
        "        )\r\n",
        "        self.add_loss(kl_loss)\r\n",
        "        return reconstructed"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2iK4UzmPt7A"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "original_dim = 784\r\n",
        "vae = VariationalAutoEncoder(original_dim, 64, 32)\r\n",
        "\r\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\r\n",
        "mse_loss_fn = tf.keras.losses.MeanSquaredError()\r\n",
        "\r\n",
        "loss_metric = tf.keras.metrics.Mean()\r\n",
        "\r\n",
        "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\r\n",
        "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\r\n",
        "\r\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\r\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\r\n",
        "\r\n",
        "epochs = 2\r\n",
        "\r\n",
        "# Iterate over epochs.\r\n",
        "for epoch in range(epochs):\r\n",
        "    print(\"Start of epoch %d\" % (epoch,))\r\n",
        "\r\n",
        "    # Iterate over the batches of the dataset.\r\n",
        "    for step, x_batch_train in enumerate(train_dataset):\r\n",
        "        with tf.GradientTape() as tape:\r\n",
        "            reconstructed = vae(x_batch_train)\r\n",
        "            # Compute reconstruction loss\r\n",
        "            loss = mse_loss_fn(x_batch_train, reconstructed)\r\n",
        "            loss += sum(vae.losses)  # Add KLD regularization loss\r\n",
        "\r\n",
        "        grads = tape.gradient(loss, vae.trainable_weights)\r\n",
        "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\r\n",
        "\r\n",
        "        loss_metric(loss)\r\n",
        "\r\n",
        "        if step % 100 == 0:\r\n",
        "            print(\"step %d: mean loss = %.4f\" % (step, loss_metric.result()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr-nhaOWUmDb"
      },
      "source": [
        "%pip install spektral"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVBNDvmgUi6p"
      },
      "source": [
        "\"\"\"\r\n",
        "This example shows how to define your own dataset and use it to train a\r\n",
        "non-trivial GNN with message-passing and pooling layers.\r\n",
        "The script also shows how to implement fast training and evaluation functions\r\n",
        "in disjoint mode, with early stopping and accuracy monitoring.\r\n",
        "The dataset that we create is a simple synthetic task in which we have random\r\n",
        "graphs with randomly-colored nodes. The goal is to classify each graph with the\r\n",
        "color that occurs the most on its nodes. For example, given a graph with 2\r\n",
        "colors and 3 nodes:\r\n",
        "x = [[1, 0],\r\n",
        "     [1, 0],\r\n",
        "     [0, 1]],\r\n",
        "the corresponding target will be [1, 0].\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import scipy.sparse as sp\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\r\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\r\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras.regularizers import l2\r\n",
        "\r\n",
        "from spektral.data import Dataset, Graph, DisjointLoader, SingleLoader\r\n",
        "from spektral.layers import GCSConv, GlobalAvgPool, GCNConv\r\n",
        "from spektral.layers.pooling import TopKPool\r\n",
        "from spektral.transforms.normalize_adj import NormalizeAdj\r\n",
        "\r\n",
        "################################################################################\r\n",
        "# PARAMETERS\r\n",
        "################################################################################\r\n",
        "learning_rate = 1e-2       # Learning rate\r\n",
        "epochs = 400               # Number of training epochs\r\n",
        "es_patience = 10           # Patience for early stopping\r\n",
        "batch_size = 32            # Batch size\r\n",
        "\r\n",
        "\r\n",
        "################################################################################\r\n",
        "# LOAD DATA\r\n",
        "################################################################################\r\n",
        "class MyDataset(Dataset):\r\n",
        "    \"\"\"\r\n",
        "    A dataset of random colored graphs.\r\n",
        "    The task is to classify each graph with the color which occurs the most in\r\n",
        "    its nodes.\r\n",
        "    The graphs have `n_colors` colors, of at least `n_min` and at most `n_max`\r\n",
        "    nodes connected with probability `p`.\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, n_samples, n_colors=3, n_min=10, n_max=100, p=0.1, **kwargs):\r\n",
        "        self.n_samples = n_samples\r\n",
        "        self.n_colors = n_colors\r\n",
        "        self.n_min = n_min\r\n",
        "        self.n_max = n_max\r\n",
        "        self.p = p\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "    def read(self):\r\n",
        "        def make_graph():\r\n",
        "            n = np.random.randint(self.n_min, self.n_max)\r\n",
        "            colors = np.random.randint(0, self.n_colors, size=n)\r\n",
        "\r\n",
        "            # Node features\r\n",
        "            x = np.zeros((n, self.n_colors))\r\n",
        "            x[np.arange(n), colors] = 1\r\n",
        "\r\n",
        "            # Edges\r\n",
        "            a = np.random.rand(n, n) <= self.p\r\n",
        "            a = np.maximum(a, a.T).astype(int)\r\n",
        "            a = sp.csr_matrix(a)\r\n",
        "\r\n",
        "            # Labels\r\n",
        "            y = np.zeros((self.n_colors, ))\r\n",
        "            color_counts = x.sum(0)\r\n",
        "            y[np.argmax(color_counts)] = 1\r\n",
        "\r\n",
        "            return Graph(x=x,a=a)\r\n",
        "\r\n",
        "        # We must return a list of Graph objects\r\n",
        "        return [make_graph() for _ in range(self.n_samples)]\r\n",
        "\r\n",
        "\r\n",
        "dataset = MyDataset(1000, transforms=NormalizeAdj())\r\n",
        "\r\n",
        "# Parameters\r\n",
        "F = dataset.n_node_features  # Dimension of node features\r\n",
        "n_out = dataset.n_labels     # Dimension of the target\r\n",
        "\r\n",
        "# Train/valid/test split\r\n",
        "idxs = np.random.permutation(len(dataset))\r\n",
        "split_va, split_te = int(0.8 * len(dataset)), int(0.9 * len(dataset))\r\n",
        "idx_tr, idx_va, idx_te = np.split(idxs, [split_va, split_te])\r\n",
        "dataset_tr = dataset[idx_tr]\r\n",
        "dataset_va = dataset[idx_va]\r\n",
        "dataset_te = dataset[idx_te]\r\n",
        "\r\n",
        "loader_tr = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=epochs)\r\n",
        "loader_va = DisjointLoader(dataset_va, batch_size=batch_size)\r\n",
        "loader_te = DisjointLoader(dataset_te, batch_size=batch_size)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNPH-h42Urby"
      },
      "source": [
        "# Parameters\r\n",
        "channels = 16          # Number of channels in the first layer\r\n",
        "dropout = 0.5          # Dropout rate for the features\r\n",
        "l2_reg = 5e-4 / 2      # L2 regularization rate\r\n",
        "learning_rate = 1e-2   # Learning rate\r\n",
        "epochs = 200           # Number of training epochs\r\n",
        "patience = 10          # Patience for early stopping\r\n",
        "a_dtype = dataset[0].a.dtype  # Only needed for TF 2.1\r\n",
        "\r\n",
        "N = 51          # Number of nodes in the graph\r\n",
        "F = 5           # Original size of node features\r\n",
        "y = 5           # Label\r\n",
        "\r\n",
        "# GNN Class Definition\r\n",
        "class GNN(Model):\r\n",
        "  def __init__(self):\r\n",
        "    super(GNN,self).__init__()\r\n",
        "    # self.x_in = Input(shape=(F,))\r\n",
        "    # self.a_in = Input((N,), sparse=True, dtype=a_dtype)\r\n",
        "    self.do_1 = Dropout(dropout)\r\n",
        "    self.gc_1 = GCNConv(channels=channels,\r\n",
        "                   activation='relu',\r\n",
        "                   kernel_regularizer=l2(l2_reg),\r\n",
        "                   use_bias=False\r\n",
        "                  )\r\n",
        "    self.do_2 = Dropout(dropout)\r\n",
        "    self.gc_2 = GCNConv(channels=F,\r\n",
        "                       activation='softmax',\r\n",
        "                       use_bias=False\r\n",
        "                       )\r\n",
        "  def call(self, inputs):\r\n",
        "      # x_in = self.x_in(inputs)\r\n",
        "      # a_in = self.a_in(inputs)\r\n",
        "      print(inputs)\r\n",
        "      x_in = inputs[0]\r\n",
        "      a_in = inputs[1]\r\n",
        "      x = self.do_1(x_in)\r\n",
        "      x = self.gc_1([x, a_in])\r\n",
        "      x = self.do_2(x)\r\n",
        "      return self.gc_2([x, a_in])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljUT9BAqVIwq"
      },
      "source": [
        "gnn = GNN()\r\n",
        "gnn.compile(optimizer=Adam(learning_rate=learning_rate), loss=tf.keras.losses.MeanSquaredError(), metrics=['mse'])\r\n",
        "loader = SingleLoader([dataset[0]])\r\n",
        "gnn.fit(loader.load(), steps_per_epoch=loader.steps_per_epoch, epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuuHDXBVaa-E"
      },
      "source": [
        "\"\"\"\r\n",
        "This example shows how to define your own dataset and use it to train a\r\n",
        "non-trivial GNN with message-passing and pooling layers.\r\n",
        "The script also shows how to implement fast training and evaluation functions\r\n",
        "in disjoint mode, with early stopping and accuracy monitoring.\r\n",
        "The dataset that we create is a simple synthetic task in which we have random\r\n",
        "graphs with randomly-colored nodes. The goal is to classify each graph with the\r\n",
        "color that occurs the most on its nodes. For example, given a graph with 2\r\n",
        "colors and 3 nodes:\r\n",
        "x = [[1, 0],\r\n",
        "     [1, 0],\r\n",
        "     [0, 1]],\r\n",
        "the corresponding target will be [1, 0].\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import scipy.sparse as sp\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.layers import Input, Dense\r\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\r\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "\r\n",
        "from spektral.data import Dataset, Graph, SingleLoader\r\n",
        "from spektral.layers import GCSConv, GlobalAvgPool\r\n",
        "from spektral.layers.pooling import TopKPool\r\n",
        "from spektral.transforms.normalize_adj import NormalizeAdj\r\n",
        "\r\n",
        "################################################################################\r\n",
        "# PARAMETERS\r\n",
        "################################################################################\r\n",
        "learning_rate = 1e-2       # Learning rate\r\n",
        "epochs = 400               # Number of training epochs\r\n",
        "es_patience = 10           # Patience for early stopping\r\n",
        "batch_size = 32            # Batch size\r\n",
        "\r\n",
        "\r\n",
        "################################################################################\r\n",
        "# LOAD DATA\r\n",
        "################################################################################\r\n",
        "class MyDataset(Dataset):\r\n",
        "    \"\"\"\r\n",
        "    A dataset of random colored graphs.\r\n",
        "    The task is to classify each graph with the color which occurs the most in\r\n",
        "    its nodes.\r\n",
        "    The graphs have `n_colors` colors, of at least `n_min` and at most `n_max`\r\n",
        "    nodes connected with probability `p`.\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, n_samples, n_colors=3, n_min=10, n_max=100, p=0.1, **kwargs):\r\n",
        "        self.n_samples = n_samples\r\n",
        "        self.n_colors = n_colors\r\n",
        "        self.n_min = n_min\r\n",
        "        self.n_max = n_max\r\n",
        "        self.p = p\r\n",
        "        super().__init__(**kwargs)\r\n",
        "\r\n",
        "    def read(self):\r\n",
        "        def make_graph():\r\n",
        "            n = np.random.randint(self.n_min, self.n_max)\r\n",
        "            colors = np.random.randint(0, self.n_colors, size=n)\r\n",
        "\r\n",
        "            # Node features\r\n",
        "            x = np.zeros((n, self.n_colors))\r\n",
        "            x[np.arange(n), colors] = 1\r\n",
        "\r\n",
        "            # Edges\r\n",
        "            a = np.random.rand(n, n) <= self.p\r\n",
        "            a = np.maximum(a, a.T).astype(int)\r\n",
        "            a = sp.csr_matrix(a)\r\n",
        "\r\n",
        "            # Labels\r\n",
        "            y = np.zeros((self.n_colors, ))\r\n",
        "            color_counts = x.sum(0)\r\n",
        "            y[np.argmax(color_counts)] = 1\r\n",
        "\r\n",
        "            return Graph(x=x, a=a, y=y)\r\n",
        "\r\n",
        "        # We must return a list of Graph objects\r\n",
        "        return [make_graph() for _ in range(self.n_samples)]\r\n",
        "\r\n",
        "\r\n",
        "dataset = MyDataset(1000, transforms=NormalizeAdj())\r\n",
        "\r\n",
        "# Parameters\r\n",
        "F = dataset.n_node_features  # Dimension of node features\r\n",
        "n_out = dataset.n_labels     # Dimension of the target\r\n",
        "\r\n",
        "# Train/valid/test split\r\n",
        "idxs = np.random.permutation(len(dataset))\r\n",
        "split_va, split_te = int(0.8 * len(dataset)), int(0.9 * len(dataset))\r\n",
        "idx_tr, idx_va, idx_te = np.split(idxs, [split_va, split_te])\r\n",
        "dataset_tr = dataset[idx_tr]\r\n",
        "dataset_va = dataset[idx_va]\r\n",
        "dataset_te = dataset[idx_te]\r\n",
        "\r\n",
        "loader_tr = SingleLoader([dataset_tr[0]], epochs=epochs)\r\n",
        "loader_va = SingleLoader([dataset_va[0]])\r\n",
        "loader_te = SingleLoader([dataset_te[0]])\r\n",
        "\r\n",
        "################################################################################\r\n",
        "# BUILD (unnecessarily big) MODEL\r\n",
        "################################################################################\r\n",
        "X_in = Input(shape=(F, ), name='X_in')\r\n",
        "A_in = Input(shape=(None,), sparse=True)\r\n",
        "I_in = Input(shape=(), name='segment_ids_in', dtype=tf.int32)\r\n",
        "\r\n",
        "X_1 = GCSConv(32, activation='relu')([X_in, A_in])\r\n",
        "X_1, A_1, I_1 = TopKPool(ratio=0.5)([X_1, A_in, I_in])\r\n",
        "X_2 = GCSConv(32, activation='relu')([X_1, A_1])\r\n",
        "X_2, A_2, I_2 = TopKPool(ratio=0.5)([X_2, A_1, I_1])\r\n",
        "X_3 = GCSConv(32, activation='relu')([X_2, A_2])\r\n",
        "X_3 = GlobalAvgPool()([X_3, I_2])\r\n",
        "output = Dense(n_out, activation='softmax')(X_3)\r\n",
        "\r\n",
        "# Build model\r\n",
        "model = Model(inputs=[X_in, A_in, I_in], outputs=output)\r\n",
        "opt = Adam(lr=learning_rate)\r\n",
        "loss_fn = CategoricalCrossentropy()\r\n",
        "acc_fn = CategoricalAccuracy()\r\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdTh3LxbadED"
      },
      "source": [
        "model.compile(optimizer=opt, loss=loss_fn, metrics=acc_fn)\r\n",
        "model.fit(loader_tr.load(), steps_per_epoch=loader_tr.steps_per_epoch, epochs=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31Wp7PkezwMy",
        "outputId": "5829709a-770c-48c6-f5e0-aca8979dca26"
      },
      "source": [
        "import spektral\r\n",
        "MPNN = spektral.layers.MessagePassing(aggregate='sum')\r\n",
        "a = np.array([[0,1,0,1,1],[1,0,1,0,0],[0,1,0,0,0],[1,0,0,0,0],[1,0,0,0,0]])\r\n",
        "print(a)\r\n",
        "x = np.array([[1,1],[1,2],[2,4],[3,2],[3,1]])\r\n",
        "a = sp.csc_matrix(a)\r\n",
        "#t = MPNN.propagate(x,a)\r\n",
        "print(a.indices)\r\n",
        "print(a)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 0 1 1]\n",
            " [1 0 1 0 0]\n",
            " [0 1 0 0 0]\n",
            " [1 0 0 0 0]\n",
            " [1 0 0 0 0]]\n",
            "[1 3 4 0 2 1 0 0]\n",
            "  (1, 0)\t1\n",
            "  (3, 0)\t1\n",
            "  (4, 0)\t1\n",
            "  (0, 1)\t1\n",
            "  (2, 1)\t1\n",
            "  (1, 2)\t1\n",
            "  (0, 3)\t1\n",
            "  (0, 4)\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCm4v73uPJfT",
        "outputId": "6e244274-0f54-4ade-f91c-4d73b44a14f0"
      },
      "source": [
        "n = 5\r\n",
        "a = np.random.rand(n, n) <= 0.6\r\n",
        "print(a.shape)\r\n",
        "a = np.maximum(a, a.T).astype(int)\r\n",
        "a = sp.csr_matrix(a)\r\n",
        "print(a.indices)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 5)\n",
            "[0 1 3 4 0 4 0 3 4 0 1 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Kl7MK2JTF2Y",
        "outputId": "657c6af7-4fdd-4567-b69a-0dde1ec0b6b2"
      },
      "source": [
        "print(a)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 0)\t1\n",
            "  (0, 1)\t1\n",
            "  (0, 3)\t1\n",
            "  (0, 4)\t1\n",
            "  (1, 0)\t1\n",
            "  (1, 4)\t1\n",
            "  (3, 0)\t1\n",
            "  (3, 3)\t1\n",
            "  (3, 4)\t1\n",
            "  (4, 0)\t1\n",
            "  (4, 1)\t1\n",
            "  (4, 3)\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKKvny3fuAGM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}