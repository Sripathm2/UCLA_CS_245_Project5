{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM1C5CHwPYTgLO+ylHLeUfo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sripathm2/UCLA_CS_245_Project5/blob/GNN/GNN/MPNN%2BLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oTGuos50JVi"
      },
      "source": [
        "%pip install spektral"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKKvny3fuAGM",
        "outputId": "6a7f834b-0880-4438-ae85-2e1cbbed08d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\r\n",
        "from scipy import sparse\r\n",
        "import tensorflow as tf\r\n",
        "import spektral\r\n",
        "from spektral.layers.ops import sp_matrix_to_sp_tensor\r\n",
        "from spektral.datasets.mnist import MNIST\r\n",
        "\r\n",
        "data = MNIST()\r\n",
        "\r\n",
        "\r\n",
        "class Net(tf.keras.Model):\r\n",
        "    def __init__(self, window=6, dropout=.5, **kwargs):\r\n",
        "        \"\"\"\r\n",
        "        Window: int. Window of days\r\n",
        "        #LSTM hidden states: 64\r\n",
        "        Training: 500 epocs, batchsize 8, Adam optimizer, LR 10-3\r\n",
        "        \"\"\"\r\n",
        "        super().__init__(**kwargs)\r\n",
        "        self._nets = self.build_MPNN_unit(dropout)\r\n",
        "        self.permute = tf.keras.layers.Permute((2,1,3))\r\n",
        "        self.flatten = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())\r\n",
        "        self.LSTM1 = tf.keras.layers.LSTM(52, return_sequences=True)\r\n",
        "        self.LSTM2 = tf.keras.layers.LSTM(52, return_sequences=False,\r\n",
        "                                          return_state=True)\r\n",
        "\r\n",
        "    def build_MPNN_unit(self, dropout):\r\n",
        "        L1 = []\r\n",
        "        L1.append(\r\n",
        "            spektral.layers.MessagePassing(aggregate='sum',\r\n",
        "                                           activation='relu')\r\n",
        "            )\r\n",
        "        L1.append(\r\n",
        "            tf.keras.layers.BatchNormalization()\r\n",
        "            )\r\n",
        "        L1.append(\r\n",
        "            tf.keras.layers.Dropout(dropout)\r\n",
        "            )\r\n",
        "        L2 = []\r\n",
        "        L2.append(\r\n",
        "            spektral.layers.MessagePassing(aggregate='sum',\r\n",
        "                                           activation='relu')\r\n",
        "            )\r\n",
        "        L2.append(\r\n",
        "            tf.keras.layers.BatchNormalization()\r\n",
        "            )\r\n",
        "        L2.append(\r\n",
        "            tf.keras.layers.Dropout(dropout)\r\n",
        "            )\r\n",
        "        return (L1,L2)\r\n",
        "\r\n",
        "\r\n",
        "    def run_MPNN_unit(self, Adj, X):\r\n",
        "        L1, L2 = self._nets\r\n",
        "        y = None\r\n",
        "        for i in range(0,len(L1)):\r\n",
        "            if i == 0: # MessagePassing layer\r\n",
        "                y = L1[i].propagate(X, Adj)\r\n",
        "                continue\r\n",
        "            # print(i,L1[i])#, y)\r\n",
        "            y = L1[i](y)\r\n",
        "        H1 = y\r\n",
        "        for i in range(0, len(L2)):\r\n",
        "            if i == 0: # MessagePassing Layer\r\n",
        "                y = L2[i].propagate(y, Adj)\r\n",
        "                continue\r\n",
        "            y = L2[i](y)\r\n",
        "        H2 = y\r\n",
        "        return tf.concat((H1,H2), axis=1)\r\n",
        "    \r\n",
        "    def call(self, Adj, X):\r\n",
        "        H_list = []\r\n",
        "        for i in range(Adj.shape[0]):\r\n",
        "          a = sp_matrix_to_sp_tensor(Adj[i])\r\n",
        "          H = self.run_MPNN_unit(a, X[i])\r\n",
        "          H_list.append(H)\r\n",
        "        H_out = tf.expand_dims(H_list, axis=0)\r\n",
        "        print('H_out: ', H_out.shape)\r\n",
        "        #LSTM_input = self.permute(H_out)[0]\r\n",
        "        LSTM_input = self.flatten(H_out)\r\n",
        "        print('LSTM Input: ',LSTM_input.shape)\r\n",
        "        x = self.LSTM1(inputs=LSTM_input)\r\n",
        "        print('After First LSTM: ',x.shape)\r\n",
        "        x, final_memory_state, final_carry_state = self.LSTM2(inputs=x)\r\n",
        "        print('After Second LSTM: ',x.shape)\r\n",
        "        print('Feature matrix: ', X.shape)\r\n",
        "        print('Output of LSTM: ',x)\r\n",
        "        #x = X+x\r\n",
        "        #Lin?\r\n",
        "        #x = tf.keras.activations.relu(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "# Create random Adj Matrices\r\n",
        "A_list = []\r\n",
        "for i in range(286):\r\n",
        "  temp = np.zeros([52,52])\r\n",
        "  for j in range(np.random.randint(1,25)):\r\n",
        "    r = np.random.randint(0,52)\r\n",
        "    s = np.random.randint(0,52)\r\n",
        "    n = np.random.randn()\r\n",
        "    temp[r,s] = n\r\n",
        "    temp[s,r] = n\r\n",
        "  A_list.append(temp)\r\n",
        "Adj = tf.expand_dims(A_list, axis=0)[0]\r\n",
        "\r\n",
        "# Create random node feature matrices\r\n",
        "X = [np.random.rand(52, 5) for i in range(0,286)]\r\n",
        "X = tf.expand_dims(X, axis=0)[0]\r\n",
        "\r\n",
        "# Create random labels\r\n",
        "y = np.random.rand(52, 1)\r\n",
        "\r\n",
        "\r\n",
        "mod = Net(window=6)\r\n",
        "mod(Adj, X)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer net_51 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "WARNING:tensorflow:Layer batch_normalization_212 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "H_out:  (1, 286, 52, 10)\n",
            "LSTM Input:  (1, 286, 520)\n",
            "After First LSTM:  (1, 286, 52)\n",
            "After Second LSTM:  (1, 52)\n",
            "Feature matrix:  (286, 52, 5)\n",
            "Output of LSTM:  tf.Tensor(\n",
            "[[-0.2356465  -0.09460261 -0.08097994 -0.12809427 -0.09205884  0.01028476\n",
            "   0.26044554 -0.10897244 -0.01098394 -0.04446738 -0.24701883 -0.10144805\n",
            "  -0.1559867   0.06473605  0.14410442  0.15812728  0.33936325  0.10017049\n",
            "   0.02381019  0.00832506 -0.14718044 -0.23101597  0.00990979 -0.16445112\n",
            "   0.24627207  0.19460455  0.20630333 -0.07012433  0.09746635  0.07180098\n",
            "   0.11246537  0.09626795  0.24672973 -0.06638323 -0.08306702 -0.1877486\n",
            "  -0.0822549   0.00179241  0.12709722  0.03083797  0.2108809   0.01050644\n",
            "  -0.02138388 -0.01025241 -0.00153005  0.10621783 -0.10933523 -0.10894186\n",
            "  -0.15470508  0.23511297  0.2111      0.19094522]], shape=(1, 52), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 52), dtype=float32, numpy=\n",
              "array([[-0.2356465 , -0.09460261, -0.08097994, -0.12809427, -0.09205884,\n",
              "         0.01028476,  0.26044554, -0.10897244, -0.01098394, -0.04446738,\n",
              "        -0.24701883, -0.10144805, -0.1559867 ,  0.06473605,  0.14410442,\n",
              "         0.15812728,  0.33936325,  0.10017049,  0.02381019,  0.00832506,\n",
              "        -0.14718044, -0.23101597,  0.00990979, -0.16445112,  0.24627207,\n",
              "         0.19460455,  0.20630333, -0.07012433,  0.09746635,  0.07180098,\n",
              "         0.11246537,  0.09626795,  0.24672973, -0.06638323, -0.08306702,\n",
              "        -0.1877486 , -0.0822549 ,  0.00179241,  0.12709722,  0.03083797,\n",
              "         0.2108809 ,  0.01050644, -0.02138388, -0.01025241, -0.00153005,\n",
              "         0.10621783, -0.10933523, -0.10894186, -0.15470508,  0.23511297,\n",
              "         0.2111    ,  0.19094522]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jisFbuBtf57H",
        "outputId": "8ebcc5c2-1555-4777-9b91-9ef0a9406595",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a_list = []\r\n",
        "for i in range(10):\r\n",
        "  temp = np.ones([5,5])*i\r\n",
        "  a_list.append(temp)\r\n",
        "aaa = tf.expand_dims(a_list, axis=0)\r\n",
        "print(aaa.shape.as_list)\r\n",
        "print(np.random.randn())"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method TensorShape.as_list of TensorShape([1, 10, 5, 5])>\n",
            "1.0518753575866997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQn3m6GqqUTc",
        "outputId": "0b9db097-51af-4cc1-a01c-592b6407483b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(aaa.shape)\r\n",
        "print(aaa)\r\n",
        "p = tf.keras.layers.TimeDistributed(tf.keras.layers.Flatten())\r\n",
        "o = p(aaa)\r\n",
        "print(o.shape)\r\n",
        "print(o)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 10, 5, 5)\n",
            "tf.Tensor(\n",
            "[[[[0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]\n",
            "   [0. 0. 0. 0. 0.]]\n",
            "\n",
            "  [[1. 1. 1. 1. 1.]\n",
            "   [1. 1. 1. 1. 1.]\n",
            "   [1. 1. 1. 1. 1.]\n",
            "   [1. 1. 1. 1. 1.]\n",
            "   [1. 1. 1. 1. 1.]]\n",
            "\n",
            "  [[2. 2. 2. 2. 2.]\n",
            "   [2. 2. 2. 2. 2.]\n",
            "   [2. 2. 2. 2. 2.]\n",
            "   [2. 2. 2. 2. 2.]\n",
            "   [2. 2. 2. 2. 2.]]\n",
            "\n",
            "  [[3. 3. 3. 3. 3.]\n",
            "   [3. 3. 3. 3. 3.]\n",
            "   [3. 3. 3. 3. 3.]\n",
            "   [3. 3. 3. 3. 3.]\n",
            "   [3. 3. 3. 3. 3.]]\n",
            "\n",
            "  [[4. 4. 4. 4. 4.]\n",
            "   [4. 4. 4. 4. 4.]\n",
            "   [4. 4. 4. 4. 4.]\n",
            "   [4. 4. 4. 4. 4.]\n",
            "   [4. 4. 4. 4. 4.]]\n",
            "\n",
            "  [[5. 5. 5. 5. 5.]\n",
            "   [5. 5. 5. 5. 5.]\n",
            "   [5. 5. 5. 5. 5.]\n",
            "   [5. 5. 5. 5. 5.]\n",
            "   [5. 5. 5. 5. 5.]]\n",
            "\n",
            "  [[6. 6. 6. 6. 6.]\n",
            "   [6. 6. 6. 6. 6.]\n",
            "   [6. 6. 6. 6. 6.]\n",
            "   [6. 6. 6. 6. 6.]\n",
            "   [6. 6. 6. 6. 6.]]\n",
            "\n",
            "  [[7. 7. 7. 7. 7.]\n",
            "   [7. 7. 7. 7. 7.]\n",
            "   [7. 7. 7. 7. 7.]\n",
            "   [7. 7. 7. 7. 7.]\n",
            "   [7. 7. 7. 7. 7.]]\n",
            "\n",
            "  [[8. 8. 8. 8. 8.]\n",
            "   [8. 8. 8. 8. 8.]\n",
            "   [8. 8. 8. 8. 8.]\n",
            "   [8. 8. 8. 8. 8.]\n",
            "   [8. 8. 8. 8. 8.]]\n",
            "\n",
            "  [[9. 9. 9. 9. 9.]\n",
            "   [9. 9. 9. 9. 9.]\n",
            "   [9. 9. 9. 9. 9.]\n",
            "   [9. 9. 9. 9. 9.]\n",
            "   [9. 9. 9. 9. 9.]]]], shape=(1, 10, 5, 5), dtype=float64)\n",
            "WARNING:tensorflow:Layer time_distributed_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "(1, 10, 25)\n",
            "tf.Tensor(\n",
            "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0.]\n",
            "  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            "   1. 1.]\n",
            "  [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            "   2. 2.]\n",
            "  [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
            "   3. 3.]\n",
            "  [4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
            "   4. 4.]\n",
            "  [5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.\n",
            "   5. 5.]\n",
            "  [6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6. 6.\n",
            "   6. 6.]\n",
            "  [7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7. 7.\n",
            "   7. 7.]\n",
            "  [8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8. 8.\n",
            "   8. 8.]\n",
            "  [9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9. 9.\n",
            "   9. 9.]]], shape=(1, 10, 25), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlzyt3qbse9r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}